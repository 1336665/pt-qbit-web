#!/usr/bin/env python3
"""
RSSè‡ªåŠ¨æŠ“å–å¼•æ“ v2.0

æ–°ç‰¹æ€§:
- å‚è€ƒVERTEXçš„æ–°ç§æŠ“å–é€»è¾‘ï¼šåªæŠ“å–çœŸæ­£çš„"æ–°ç§å­"
- æ”¯æŒé…ç½®æœ€å¤§ç§å­å¹´é¾„ï¼ˆåˆ†é’Ÿçº§åˆ«ï¼Œé»˜è®¤10åˆ†é’Ÿï¼‰
- é¦–æ¬¡è¿è¡Œæ¨¡å¼ï¼šåªè®°å½•ä¸æ·»åŠ ï¼Œé¿å…ä¸€æ¬¡æ€§æ·»åŠ å¤§é‡æ—§ç§
- å¢é‡æŠ“å–ï¼šåªæ·»åŠ ä¸Šæ¬¡æ£€æŸ¥åæ–°å‘å¸ƒçš„ç§å­
- æ”¯æŒç§å­å‘å¸ƒæ—¶é—´ç²¾ç¡®åˆ°ç§’çº§æ¯”è¾ƒ
"""

import os
import re
import time
import calendar
import json
import hashlib
import threading
import logging
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field
from collections import OrderedDict

try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False

try:
    from bs4 import BeautifulSoup
    BS4_AVAILABLE = True
except ImportError:
    BS4_AVAILABLE = False

try:
    import feedparser
    FEEDPARSER_AVAILABLE = True
except ImportError:
    FEEDPARSER_AVAILABLE = False


@dataclass
class RSSItem:
    title: str
    link: str
    torrent_url: str
    size: int = 0
    pub_date: Optional[datetime] = None
    info_hash: str = ""
    site_id: int = 0
    site_name: str = ""


@dataclass
class FetchResult:
    site_id: int
    site_name: str
    success: bool
    items_found: int = 0
    items_added: int = 0
    items_skipped: int = 0
    items_too_old: int = 0
    items_cached: int = 0
    error: str = ""
    timestamp: float = field(default_factory=time.time)
    mode: str = "normal"  # normal, first_run, incremental


class LRUCache:
    def __init__(self, capacity: int = 10000):
        self.cache: OrderedDict = OrderedDict()
        self.capacity = capacity
        self._lock = threading.Lock()
    
    def get(self, key: str) -> bool:
        with self._lock:
            if key in self.cache:
                self.cache.move_to_end(key)
                return True
            return False
    
    def put(self, key: str, timestamp: float = None):
        with self._lock:
            if key in self.cache:
                self.cache.move_to_end(key)
            else:
                if len(self.cache) >= self.capacity:
                    self.cache.popitem(last=False)
                self.cache[key] = timestamp or time.time()
    
    def clear(self):
        with self._lock:
            self.cache.clear()
    
    def size(self) -> int:
        with self._lock:
            return len(self.cache)
    
    def to_list(self) -> List[str]:
        with self._lock:
            return list(self.cache.keys())
    
    def to_dict(self) -> Dict[str, float]:
        with self._lock:
            return dict(self.cache)
    
    def load_from_list(self, items: List[str]):
        with self._lock:
            self.cache.clear()
            for item in items[-self.capacity:]:
                self.cache[item] = time.time()
    
    def load_from_dict(self, data: Dict[str, float]):
        with self._lock:
            self.cache.clear()
            items = sorted(data.items(), key=lambda x: x[1])[-self.capacity:]
            for k, v in items:
                self.cache[k] = v


class RSSEngine:
    def __init__(self, db, qb_manager, notifier=None, logger=None):
        self.db = db
        self.qb_manager = qb_manager
        self.notifier = notifier
        self.logger = logger or logging.getLogger("rss_engine")
        
        self._running = False
        self._thread = None
        self._stop_event = threading.Event()
        
        # é…ç½®
        self._fetch_interval = 300  # é»˜è®¤5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
        self._enabled = False
        self._min_free_space = 10 * 1024 * 1024 * 1024  # 10GB
        
        # VERTEXé£æ ¼é…ç½®ï¼šåªæŠ“å–åˆšå‘å¸ƒçš„ç§å­
        self._max_torrent_age_minutes = 60  # é»˜è®¤åªæŠ“å–60åˆ†é’Ÿå†…å‘å¸ƒçš„ç§å­ï¼ˆä»10æ”¹ä¸º60ï¼‰
        self._max_items_per_fetch = 10  # æ¯æ¬¡æœ€å¤šæ·»åŠ 10ä¸ªç§å­ï¼ˆä»5æ”¹ä¸º10ï¼‰
        self._first_run_mode = True  # é¦–æ¬¡è¿è¡Œä»…æ·»åŠ æœ€æ–°ç§å­ï¼ˆå—æœ€å¤§å¹´é¾„é™åˆ¶ï¼‰
        
        # ç¼“å­˜
        self._hash_cache = LRUCache(capacity=10000)
        self._last_fetch = {}  # site_id -> timestamp
        self._last_pub_date = {}  # site_id -> datetime (ä¸Šæ¬¡æŠ“å–åˆ°çš„æœ€æ–°å‘å¸ƒæ—¶é—´)
        self._fetch_results = []
        self._max_results = 100
        self._first_run_done = {}  # site_id -> bool
        
        self._session = None
        if REQUESTS_AVAILABLE:
            self._session = requests.Session()
            self._session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
        
        self._load_state()
    
    def _load_state(self):
        """åŠ è½½æŒä¹…åŒ–çŠ¶æ€"""
        try:
            # åŠ è½½hashç¼“å­˜
            cache_str = self.db.get_config('rss_hash_cache')
            if cache_str:
                try:
                    cache_data = json.loads(cache_str)
                    if isinstance(cache_data, dict):
                        self._hash_cache.load_from_dict(cache_data)
                    else:
                        self._hash_cache.load_from_list(cache_data)
                except:
                    pass
            
            # åŠ è½½ä¸Šæ¬¡å‘å¸ƒæ—¶é—´è®°å½•
            last_pub_str = self.db.get_config('rss_last_pub_date')
            if last_pub_str:
                try:
                    data = json.loads(last_pub_str)
                    for site_id, ts in data.items():
                        self._last_pub_date[int(site_id)] = datetime.fromtimestamp(ts, tz=timezone.utc)
                except:
                    pass
            
            # åŠ è½½é¦–æ¬¡è¿è¡Œæ ‡è®°
            first_run_str = self.db.get_config('rss_first_run_done')
            if first_run_str:
                try:
                    self._first_run_done = {int(k): v for k, v in json.loads(first_run_str).items()}
                except:
                    pass
                    
        except Exception as e:
            self._log('warning', f"åŠ è½½RSSçŠ¶æ€å¤±è´¥: {e}")
    
    def _save_state(self):
        """ä¿å­˜æŒä¹…åŒ–çŠ¶æ€"""
        try:
            # ä¿å­˜hashç¼“å­˜
            cache_dict = self._hash_cache.to_dict()
            self.db.set_config('rss_hash_cache', json.dumps(cache_dict))
            
            # ä¿å­˜ä¸Šæ¬¡å‘å¸ƒæ—¶é—´
            last_pub_data = {str(k): v.timestamp() for k, v in self._last_pub_date.items()}
            self.db.set_config('rss_last_pub_date', json.dumps(last_pub_data))
            
            # ä¿å­˜é¦–æ¬¡è¿è¡Œæ ‡è®°
            self.db.set_config('rss_first_run_done', json.dumps(self._first_run_done))
            
        except Exception as e:
            self._log('warning', f"ä¿å­˜RSSçŠ¶æ€å¤±è´¥: {e}")
    
    def _log(self, level: str, message: str):
        getattr(self.logger, level.lower(), self.logger.info)(message)
        try:
            self.db.add_log(level.upper(), f"[RSS] {message}")
        except:
            pass
    
    @staticmethod
    def _clean_cookie(cookie: str) -> str:
        """æ¸…ç†Cookieæ ¼å¼ï¼šå°†å¤šè¡ŒCookieåˆå¹¶ä¸ºå•è¡Œ"""
        if not cookie:
            return ''
        
        # ç§»é™¤ä¸å¯è§å­—ç¬¦
        cookie = re.sub(r'[\ufeff\ufffe\u200b\u200c\u200d\u2060\x00-\x1f\x7f-\x9f]', '', cookie)
        
        # å°†æ¢è¡Œç¬¦æ›¿æ¢ä¸ºåˆ†éš”ç¬¦
        cookie = cookie.replace('\r\n', ';').replace('\r', ';').replace('\n', ';')

        attribute_keys = {
            'path', 'domain', 'expires', 'max-age', 'secure', 'httponly', 'samesite',
        }

        if ';' not in cookie and cookie.count('=') > 1:
            cookie = re.sub(r'\s+', ';', cookie)

        # åˆ†å‰²å¹¶é‡æ–°ç»„åˆcookie
        seen_keys = set()
        parts = []
        for part in cookie.split(';'):
            part = part.strip()
            if part and '=' in part:
                key, value = part.split('=', 1)
                key = key.strip()
                value = value.strip()
                if not key or key.lower() in attribute_keys:
                    continue
                if key not in seen_keys:
                    seen_keys.add(key)
                    parts.append(f"{key}={value}")
        
        return '; '.join(parts)
    
    @staticmethod
    def _clean_url(url: str) -> str:
        """æ¸…ç†URLï¼šç§»é™¤ä¸å¯è§å­—ç¬¦å’Œå‰åç©ºç™½"""
        if not url:
            return ''
        
        # ç§»é™¤ä¸å¯è§å­—ç¬¦
        url = re.sub(r'[\ufeff\ufffe\u200b\u200c\u200d\u2060\x00-\x1f\x7f-\x9f]', '', url)
        
        # ç§»é™¤å‰åç©ºç™½
        url = url.strip().strip('\u3000')
        
        return url

    @staticmethod
    def _to_utc(dt: Optional[datetime]) -> Optional[datetime]:
        if not dt:
            return None
        if dt.tzinfo is None:
            return dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(timezone.utc)
    
    def start(self):
        if self._running:
            return
        
        self._enabled = self.db.get_config('rss_fetch_enabled') == 'true'
        try:
            self._fetch_interval = int(self.db.get_config('rss_fetch_interval') or 300)
        except:
            self._fetch_interval = 300
        
        # åŠ è½½æœ€å¤§ç§å­å¹´é¾„é…ç½®
        try:
            self._max_torrent_age_minutes = int(self.db.get_config('rss_max_age_minutes') or 60)
        except:
            self._max_torrent_age_minutes = 60
        
        if not self._enabled:
            self._log('info', "RSSå¼•æ“å·²ç¦ç”¨")
            return
        
        self._running = True
        self._stop_event.clear()
        self._thread = threading.Thread(target=self._worker, daemon=True, name="RSS-Engine")
        self._thread.start()
        self._log('info', f"RSSå¼•æ“å·²å¯åŠ¨ (é—´éš”: {self._fetch_interval}ç§’, æœ€å¤§å¹´é¾„: {self._max_torrent_age_minutes}åˆ†é’Ÿ)")
    
    def stop(self):
        self._running = False
        self._stop_event.set()
        self._save_state()
        self._log('info', "RSSå¼•æ“å·²åœæ­¢")
    
    def enable(self):
        self._enabled = True
        self.db.set_config('rss_fetch_enabled', 'true')
        if not self._running:
            self.start()
    
    def disable(self):
        self._enabled = False
        self.db.set_config('rss_fetch_enabled', 'false')
    
    def set_interval(self, seconds: int):
        seconds = max(60, min(3600, seconds))
        self._fetch_interval = seconds
        self.db.set_config('rss_fetch_interval', str(seconds))
    
    def set_max_age(self, minutes: int):
        """è®¾ç½®æœ€å¤§ç§å­å¹´é¾„ï¼ˆåˆ†é’Ÿï¼‰"""
        minutes = max(1, min(1440, minutes))  # 1åˆ†é’Ÿåˆ°24å°æ—¶
        self._max_torrent_age_minutes = minutes
        self.db.set_config('rss_max_age_minutes', str(minutes))
    
    def fetch_now(self, site_id: int = None) -> List[FetchResult]:
        return self._do_fetch(site_id)
    
    def clear_cache(self):
        self._hash_cache.clear()
        self._last_pub_date.clear()
        self._first_run_done.clear()
        self.db.set_config('rss_hash_cache', '{}')
        self.db.set_config('rss_last_pub_date', '{}')
        self.db.set_config('rss_first_run_done', '{}')
        self._log('info', "å·²æ¸…é™¤RSSç¼“å­˜å’ŒçŠ¶æ€")
    
    def get_status(self) -> Dict[str, Any]:
        sites = self.db.get_pt_sites_with_rss()
        return {
            'enabled': self._enabled,
            'running': self._running,
            'fetch_interval': self._fetch_interval,
            'max_age_minutes': self._max_torrent_age_minutes,
            'cache_size': self._hash_cache.size(),
            'sites_count': len(sites),
            'sites': [{'id': s['id'], 'name': s['name'], 
                      'first_run_done': self._first_run_done.get(s['id'], False),
                      'last_fetch': self._last_fetch.get(s['id'])}
                     for s in sites],
            'last_fetch': self._last_fetch,
        }
    
    def get_results(self, limit: int = 50) -> List[Dict]:
        results = self._fetch_results[-limit:]
        return [{
            'site_id': r.site_id,
            'site_name': r.site_name,
            'success': r.success,
            'items_found': r.items_found,
            'items_added': r.items_added,
            'items_skipped': r.items_skipped,
            'items_too_old': r.items_too_old,
            'items_cached': r.items_cached,
            'mode': r.mode,
            'error': r.error,
            'time': datetime.fromtimestamp(r.timestamp).strftime('%Y-%m-%d %H:%M:%S'),
            'time_str': datetime.fromtimestamp(r.timestamp).strftime('%H:%M:%S')
        } for r in reversed(results)]
    
    def _worker(self):
        while self._running and not self._stop_event.is_set():
            try:
                self._enabled = self.db.get_config('rss_fetch_enabled') == 'true'
                try:
                    self._fetch_interval = int(self.db.get_config('rss_fetch_interval') or 300)
                    self._max_torrent_age_minutes = int(self.db.get_config('rss_max_age_minutes') or 10)
                except:
                    pass
                
                if self._enabled:
                    self._do_fetch()
            except Exception as e:
                self._log('error', f"RSSæŠ“å–å¼‚å¸¸: {e}")
            
            self._stop_event.wait(self._fetch_interval)
    
    def _do_fetch(self, site_id: int = None) -> List[FetchResult]:
        results = []
        
        if not REQUESTS_AVAILABLE:
            return results
        
        sites = self.db.get_pt_sites_with_rss()
        
        if site_id:
            sites = [s for s in sites if s['id'] == site_id]
        
        for site in sites:
            result = self._fetch_site(site)
            results.append(result)
            self._fetch_results.append(result)
            
            if len(self._fetch_results) > self._max_results:
                self._fetch_results = self._fetch_results[-self._max_results:]
        
        self._save_state()
        return results
    
    def _fetch_site(self, site: dict) -> FetchResult:
        site_id = site['id']
        site_name = site['name']
        rss_url = site.get('rss_url', '')
        cookie = site.get('cookie', '')
        
        # æ¸…ç†URLå’ŒCookieï¼ˆé˜²æ­¢æ ¼å¼é—®é¢˜ï¼‰
        rss_url = self._clean_url(rss_url)
        cookie = self._clean_cookie(cookie)
        
        # åˆ¤æ–­æ˜¯å¦é¦–æ¬¡è¿è¡Œ
        is_first_run = not self._first_run_done.get(site_id, False)
        
        result = FetchResult(
            site_id=site_id, 
            site_name=site_name, 
            success=False,
            mode='first_run' if is_first_run else 'incremental'
        )
        
        if not rss_url:
            result.error = "æœªé…ç½®RSS URL"
            return result
        
        try:
            headers = {}
            # æ³¨æ„ï¼šå¤§å¤šæ•°PTç«™RSSé“¾æ¥å·²åŒ…å«passkeyï¼Œä¸éœ€è¦Cookie
            # Cookieä¸»è¦ç”¨äºä¸‹è½½.torrentæ–‡ä»¶æ—¶çš„è®¤è¯
            if cookie:
                headers['Cookie'] = cookie
            
            mode_str = "é¦–æ¬¡è¿è¡Œ(ä»…æ–°å¢æœ€æ–°)" if is_first_run else "å¢é‡æŠ“å–"
            self._log('info', f"[{site_name}] å¼€å§‹{mode_str}...")
            
            resp = self._session.get(rss_url, headers=headers, timeout=30)
            resp.raise_for_status()

            content_type = resp.headers.get('content-type', '')
            body_preview = resp.text.lstrip()[:200].lower() if resp.text else ''
            if 'html' in content_type.lower() and 'xml' not in content_type.lower():
                result.error = "RSSè¿”å›HTMLï¼Œå¯èƒ½éœ€è¦ç™»å½•æˆ–Cookieæ— æ•ˆ"
                return result
            if body_preview.startswith('<!doctype html') or body_preview.startswith('<html'):
                result.error = "RSSè¿”å›HTMLï¼Œå¯èƒ½éœ€è¦ç™»å½•æˆ–Cookieæ— æ•ˆ"
                return result
            
            items = self._parse_rss(resp.text, site)
            result.items_found = len(items)
            
            if not items:
                result.success = True
                self._log('info', f"[{site_name}] RSSä¸ºç©º")
                return result
            
            now = datetime.now(timezone.utc)
            max_age_seconds = self._max_torrent_age_minutes * 60
            
            # è·å–ä¸Šæ¬¡æŠ“å–åˆ°çš„æœ€æ–°å‘å¸ƒæ—¶é—´
            last_pub = self._to_utc(self._last_pub_date.get(site_id))
            
            added = 0
            skipped = 0
            too_old = 0
            cached = 0
            newest_pub_date = None
            
            # itemså·²æŒ‰å‘å¸ƒæ—¶é—´å€’åºæ’åˆ—ï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
            for item in items:
                # æ›´æ–°æœ€æ–°å‘å¸ƒæ—¶é—´
                item.pub_date = self._to_utc(item.pub_date)
                if item.pub_date and (newest_pub_date is None or item.pub_date > newest_pub_date):
                    newest_pub_date = item.pub_date
                
                # ç”Ÿæˆå”¯ä¸€æ ‡è¯†
                hash_key = item.info_hash or hashlib.md5(item.torrent_url.encode()).hexdigest()
                
                # æ£€æŸ¥æ˜¯å¦å·²åœ¨ç¼“å­˜ä¸­
                if self._hash_cache.get(hash_key):
                    cached += 1
                    continue
                
                # é¦–æ¬¡è¿è¡Œæ¨¡å¼ï¼šåªæ–°å¢æœ€æ–°ç§å­ï¼ˆæœ‰å‘å¸ƒæ—¶é—´åˆ™å—æœ€å¤§å¹´é¾„é™åˆ¶ï¼‰
                if is_first_run and self._first_run_mode:
                    if item.pub_date:
                        age_seconds = (now - item.pub_date).total_seconds()
                        if age_seconds > max_age_seconds:
                            too_old += 1
                            self._hash_cache.put(hash_key, time.time())
                            continue
                
                # æ£€æŸ¥ç§å­å¹´é¾„
                if item.pub_date:
                    age_seconds = (now - item.pub_date).total_seconds()
                    
                    # è·³è¿‡å¤ªæ—§çš„ç§å­
                    if age_seconds > max_age_seconds:
                        too_old += 1
                        # ä¹Ÿè®°å½•åˆ°ç¼“å­˜ï¼Œé¿å…ä¸‹æ¬¡å†æ£€æŸ¥
                        self._hash_cache.put(hash_key, time.time())
                        continue
                    
                    # å¢é‡æ¨¡å¼ï¼šåªæ·»åŠ æ¯”ä¸Šæ¬¡è®°å½•æ›´æ–°çš„ç§å­
                    if last_pub and item.pub_date <= last_pub:
                        skipped += 1
                        self._hash_cache.put(hash_key, time.time())
                        continue
                
                # é™åˆ¶æ¯æ¬¡æ·»åŠ æ•°é‡
                if added >= self._max_items_per_fetch:
                    self._log('info', f"[{site_name}] å·²è¾¾åˆ°å•æ¬¡æ·»åŠ ä¸Šé™ ({self._max_items_per_fetch})")
                    break
                
                # é€‰æ‹©å®ä¾‹å¹¶æ·»åŠ 
                instance = self._select_best_instance(item.size, site)
                if not instance:
                    skipped += 1
                    self._hash_cache.put(hash_key, time.time())
                    continue
                
                if self._add_torrent(instance, item, cookie):
                    self._hash_cache.put(hash_key, time.time())
                    added += 1
                    
                    # è®¡ç®—å¹´é¾„å­—ç¬¦ä¸²
                    age_str = ""
                    if item.pub_date:
                        age_seconds = (now - item.pub_date).total_seconds()
                        if age_seconds < 60:
                            age_str = f" ({int(age_seconds)}ç§’å‰)"
                        elif age_seconds < 3600:
                            age_str = f" ({int(age_seconds / 60)}åˆ†é’Ÿå‰)"
                        else:
                            age_str = f" ({int(age_seconds / 3600)}å°æ—¶å‰)"
                    
                    self._log('info', f"[{site_name}] âœ… æ·»åŠ : {item.title[:40]}{age_str}")
                    
                    # å‘é€é€šçŸ¥
                    if self.notifier:
                        try:
                            self.notifier.notify(
                                f"ğŸ†• æ–°ç§å­æ·»åŠ \nç«™ç‚¹: {site_name}\nåç§°: {item.title[:50]}\nå®ä¾‹: {instance['name']}"
                            )
                        except:
                            pass
                else:
                    skipped += 1
                    self._hash_cache.put(hash_key, time.time())
            
            # æ›´æ–°æœ€æ–°å‘å¸ƒæ—¶é—´è®°å½•
            if newest_pub_date:
                self._last_pub_date[site_id] = newest_pub_date
            
            # é¦–æ¬¡è¿è¡Œå®Œæˆï¼Œæ ‡è®°
            if is_first_run:
                self._first_run_done[site_id] = True
                self._log('info', f"[{site_name}] é¦–æ¬¡è¿è¡Œå®Œæˆï¼Œå·²è®°å½• {len(items)} ä¸ªç§å­hash")
            
            result.items_added = added
            result.items_skipped = skipped
            result.items_too_old = too_old
            result.items_cached = cached
            result.success = True
            self._last_fetch[site_id] = time.time()
            
            # æ—¥å¿—
            log_parts = [f"å‘ç°{len(items)}ä¸ª"]
            if added > 0:
                log_parts.append(f"æ·»åŠ {added}ä¸ª")
            if too_old > 0:
                log_parts.append(f"è¿‡æ—§{too_old}ä¸ª")
            if cached > 0:
                log_parts.append(f"å·²ç¼“å­˜{cached}ä¸ª")
            if skipped > 0:
                log_parts.append(f"è·³è¿‡{skipped}ä¸ª")
            self._log('info', f"[{site_name}] å®Œæˆ: {', '.join(log_parts)}")
            
        except requests.exceptions.Timeout:
            result.error = "è¯·æ±‚è¶…æ—¶"
        except requests.exceptions.RequestException as e:
            result.error = f"è¯·æ±‚å¤±è´¥: {str(e)[:50]}"
        except Exception as e:
            result.error = f"è§£æå¤±è´¥: {str(e)[:50]}"
            self._log('error', f"[{site_name}] é”™è¯¯: {e}")
        
        return result
    
    def _parse_rss(self, content: str, site: dict) -> List[RSSItem]:
        items = []
        
        # æ¸…ç†å†…å®¹å¼€å¤´çš„BOMå’Œç©ºç™½å­—ç¬¦
        content = content.lstrip('\ufeff\ufffe')  # ç§»é™¤BOM
        content = content.strip()  # ç§»é™¤é¦–å°¾ç©ºç™½
        
        # ç¡®ä¿ä»¥ < å¼€å¤´ï¼ˆXMLå£°æ˜æˆ–æ ¹å…ƒç´ ï¼‰
        if content and not content.startswith('<'):
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ª < çš„ä½ç½®
            xml_start = content.find('<')
            if xml_start > 0:
                content = content[xml_start:]
        
        if FEEDPARSER_AVAILABLE:
            try:
                feed = feedparser.parse(content)
                
                # æ£€æŸ¥feedparseræ˜¯å¦æŠ¥å‘Šé”™è¯¯
                if feed.bozo and hasattr(feed, 'bozo_exception'):
                    bozo_msg = str(feed.bozo_exception)
                    # å¦‚æœæœ‰entriesï¼Œå¿½ç•¥éä¸¥é‡é”™è¯¯ç»§ç»­å¤„ç†
                    if not feed.entries:
                        raise Exception(f"RSSè§£æé”™è¯¯: {bozo_msg[:80]}")
                
                for entry in feed.entries:
                    pub_date = None
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        try:
                            timestamp = calendar.timegm(entry.published_parsed)
                            pub_date = datetime.fromtimestamp(timestamp, tz=timezone.utc)
                        except:
                            pass
                    
                    item = RSSItem(
                        title=entry.get('title', ''),
                        link=entry.get('link', ''),
                        torrent_url=self._extract_torrent_url(entry),
                        size=self._parse_size(entry),
                        pub_date=pub_date,
                        info_hash=self._extract_hash(entry),
                        site_id=site['id'],
                        site_name=site['name']
                    )
                    if item.torrent_url:
                        items.append(item)
                
                # æŒ‰å‘å¸ƒæ—¶é—´å€’åºæ’åˆ—ï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
                min_utc = datetime.min.replace(tzinfo=timezone.utc)
                items.sort(key=lambda x: x.pub_date or min_utc, reverse=True)
                return items
            except Exception as e:
                # è®°å½•é”™è¯¯ä½†ç»§ç»­å°è¯•XMLå›é€€
                self._log('debug', f"[{site.get('name')}] feedparserå¤±è´¥: {e}")
        
        # XML fallback
        try:
            import xml.etree.ElementTree as ET
            root = ET.fromstring(content)
            
            for item_elem in root.iter('item'):
                title = item_elem.find('title')
                link = item_elem.find('link')
                enclosure = item_elem.find('enclosure')
                pub_date_elem = item_elem.find('pubDate')
                
                torrent_url = ''
                size = 0
                pub_date = None
                
                if enclosure is not None:
                    torrent_url = enclosure.get('url', '')
                    try:
                        size = int(enclosure.get('length', 0))
                    except:
                        pass
                elif link is not None:
                    torrent_url = link.text or ''
                
                if pub_date_elem is not None and pub_date_elem.text:
                    try:
                        from email.utils import parsedate_to_datetime
                        pub_date = parsedate_to_datetime(pub_date_elem.text)
                        if pub_date.tzinfo is None:
                            pub_date = pub_date.replace(tzinfo=timezone.utc)
                        else:
                            pub_date = pub_date.astimezone(timezone.utc)
                    except:
                        pass
                
                if torrent_url:
                    items.append(RSSItem(
                        title=title.text if title is not None else '',
                        link=link.text if link is not None else '',
                        torrent_url=torrent_url,
                        size=size,
                        pub_date=pub_date,
                        site_id=site['id'],
                        site_name=site['name']
                    ))
            
            # æŒ‰å‘å¸ƒæ—¶é—´å€’åºæ’åˆ—
            min_utc = datetime.min.replace(tzinfo=timezone.utc)
            items.sort(key=lambda x: x.pub_date or min_utc, reverse=True)
        except:
            pass
        
        return items
    
    def _extract_torrent_url(self, entry) -> str:
        for link in entry.get('links', []):
            if link.get('type') == 'application/x-bittorrent':
                return link.get('href', '')
            if 'torrent' in link.get('href', '').lower():
                return link.get('href', '')
        
        link = entry.get('link', '')
        if 'torrent' in link.lower() or link.endswith('.torrent'):
            return link
        
        for enc in entry.get('enclosures', []):
            url = enc.get('url', enc.get('href', ''))
            if url:
                return url
        
        return ''
    
    def _parse_size(self, entry) -> int:
        for enc in entry.get('enclosures', []):
            try:
                return int(enc.get('length', 0))
            except:
                pass
        return 0
    
    def _extract_hash(self, entry) -> str:
        link = entry.get('link', '')
        match = re.search(r'([a-fA-F0-9]{40})', link)
        if match:
            return match.group(1).lower()
        return ''
    
    def _select_best_instance(self, torrent_size: int, site: Optional[dict] = None) -> Optional[dict]:
        instances = self.db.get_qb_instances()
        preferred_instance_id = site.get('preferred_instance_id') if site else None
        if preferred_instance_id:
            for inst in instances:
                if inst['id'] == preferred_instance_id and inst.get('enabled'):
                    if self.qb_manager.is_connected(inst['id']):
                        return inst
                    self._log('warning', f"æŒ‡å®šå®ä¾‹æœªè¿æ¥: {inst['name']}")
                    return None
            self._log('warning', f"æœªæ‰¾åˆ°æŒ‡å®šå®ä¾‹: {preferred_instance_id}")
            return None

        candidates = []
        
        for inst in instances:
            if not inst.get('enabled'):
                continue
            
            inst_id = inst['id']
            if not self.qb_manager.is_connected(inst_id):
                continue
            
            free_space = self.qb_manager.get_free_space(inst_id)
            
            required = torrent_size + self._min_free_space
            if free_space >= required:
                candidates.append({
                    'instance': inst,
                    'free_space': free_space
                })
        
        if not candidates:
            return None
        
        candidates.sort(key=lambda x: x['free_space'], reverse=True)
        return candidates[0]['instance']
    
    def _add_torrent(self, instance: dict, item: RSSItem, cookie: str = '') -> bool:
        try:
            headers = {}
            if cookie:
                # ç¡®ä¿cookieæ ¼å¼æ­£ç¡®
                cookie = self._clean_cookie(cookie)
                headers['Cookie'] = cookie
            
            # æ¸…ç†torrent URL
            torrent_url = self._clean_url(item.torrent_url)
            
            resp = self._session.get(torrent_url, headers=headers, timeout=30)
            resp.raise_for_status()
            
            content_type = resp.headers.get('content-type', '')
            if 'html' in content_type.lower():
                # å¦‚æœè¿”å›HTMLï¼Œå¯èƒ½æ˜¯éœ€è¦ç™»å½•æˆ–Cookieæ— æ•ˆ
                self._log('warning', f"ä¸‹è½½ç§å­è¿”å›HTMLï¼Œå¯èƒ½Cookieæ— æ•ˆæˆ–éœ€è¦ç™»å½•")
                return False
            
            success, msg = self.qb_manager.add_torrent(
                instance_id=instance['id'],
                torrent_file=resp.content
            )
            
            if success:
                try:
                    self.db.update_stats(total_added=1)
                except:
                    pass
            
            return success
            
        except Exception as e:
            self._log('error', f"æ·»åŠ ç§å­å¤±è´¥: {str(e)[:50]}")
            return False


def create_rss_engine(db, qb_manager, notifier=None) -> RSSEngine:
    logger = logging.getLogger("rss_engine")
    return RSSEngine(db, qb_manager, notifier, logger)
